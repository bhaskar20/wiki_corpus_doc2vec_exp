{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import os\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim import utils\n",
    "import multiprocessing\n",
    "import re\n",
    "import signal\n",
    "from pickle import PicklingError\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_MIN_LEN = 2\n",
    "TOKEN_MAX_LEN = 15\n",
    "IGNORED_NAMESPACES = [\n",
    "    'Wikipedia', 'Category', 'File', 'Portal', 'Template',\n",
    "    'MediaWiki', 'User', 'Help', 'Book', 'Draft', 'WikiProject',\n",
    "    'Special', 'Talk'\n",
    "]\n",
    "ARTICLE_MIN_WORDS = 50\n",
    "DESIRED_ARTICLE_MIN_WORDS = 5\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_P0 = re.compile(r'<!--.*?-->', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Comments.\"\"\"\n",
    "RE_P1 = re.compile(r'<ref([> ].*?)(</ref>|/>)', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Footnotes.\"\"\"\n",
    "RE_P2 = re.compile(r'(\\n\\[\\[[a-z][a-z][\\w-]*:[^:\\]]+\\]\\])+$', re.UNICODE)\n",
    "\"\"\"Links to languages.\"\"\"\n",
    "RE_P3 = re.compile(r'{{([^}{]*)}}', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Template.\"\"\"\n",
    "RE_P4 = re.compile(r'{{([^}]*)}}', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Template.\"\"\"\n",
    "RE_P5 = re.compile(r'\\[(\\w+):\\/\\/(.*?)(( (.*?))|())\\]', re.UNICODE)\n",
    "\"\"\"Remove URL, keep description.\"\"\"\n",
    "RE_P6 = re.compile(r'\\[([^][]*)\\|([^][]*)\\]', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Simplify links, keep description.\"\"\"\n",
    "RE_P7 = re.compile(r'\\n\\[\\[[iI]mage(.*?)(\\|.*?)*\\|(.*?)\\]\\]', re.UNICODE)\n",
    "\"\"\"Keep description of images.\"\"\"\n",
    "RE_P8 = re.compile(r'\\n\\[\\[[fF]ile(.*?)(\\|.*?)*\\|(.*?)\\]\\]', re.UNICODE)\n",
    "\"\"\"Keep description of files.\"\"\"\n",
    "RE_P9 = re.compile(r'<nowiki([> ].*?)(</nowiki>|/>)', re.DOTALL | re.UNICODE)\n",
    "\"\"\"External links.\"\"\"\n",
    "RE_P10 = re.compile(r'<math([> ].*?)(</math>|/>)', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Math content.\"\"\"\n",
    "RE_P11 = re.compile(r'<(.*?)>', re.DOTALL | re.UNICODE)\n",
    "\"\"\"All other tags.\"\"\"\n",
    "RE_P12 = re.compile(r'(({\\|)|(\\|-(?!\\d))|(\\|}))(.*?)(?=\\n)', re.UNICODE)\n",
    "\"\"\"Table formatting.\"\"\"\n",
    "RE_P13 = re.compile(r'(?<=(\\n[ ])|(\\n\\n)|([ ]{2})|(.\\n)|(.\\t))(\\||\\!)([^[\\]\\n]*?\\|)*', re.UNICODE)\n",
    "\"\"\"Table cell formatting.\"\"\"\n",
    "RE_P14 = re.compile(r'\\[\\[Category:[^][]*\\]\\]', re.UNICODE)\n",
    "\"\"\"Categories.\"\"\"\n",
    "RE_P15 = re.compile(r'\\[\\[([fF]ile:|[iI]mage)[^]]*(\\]\\])', re.UNICODE)\n",
    "\"\"\"Remove File and Image templates.\"\"\"\n",
    "RE_P16 = re.compile(r'\\[{2}(.*?)\\]{2}', re.UNICODE)\n",
    "\"\"\"Capture interlinks text and article linked\"\"\"\n",
    "RE_P17 = re.compile(\n",
    "    r'(\\n.{0,4}((bgcolor)|(\\d{0,1}[ ]?colspan)|(rowspan)|(style=)|(class=)|(align=)|(scope=))(.*))|'\n",
    "    r'(^.{0,2}((bgcolor)|(\\d{0,1}[ ]?colspan)|(rowspan)|(style=)|(class=)|(align=))(.*))',\n",
    "    re.UNICODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(content, token_min_len=TOKEN_MIN_LEN, token_max_len=TOKEN_MAX_LEN, lower=True):\n",
    "    \"\"\"Tokenize a piece of text from Wikipedia.\n",
    "\n",
    "    Set `token_min_len`, `token_max_len` as character length (not bytes!) thresholds for individual tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : str\n",
    "        String without markup (see :func:`~gensim.corpora.wikicorpus.filter_wiki`).\n",
    "    token_min_len : int\n",
    "        Minimal token length.\n",
    "    token_max_len : int\n",
    "        Maximal token length.\n",
    "    lower : bool\n",
    "         Convert `content` to lower case?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        List of tokens from `content`.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO maybe ignore tokens with non-latin characters? (no chinese, arabic, russian etc.)\n",
    "    return [\n",
    "        utils.to_unicode(token) for token in utils.tokenize(content, lower=lower, errors='ignore')\n",
    "        if token_min_len <= len(token) <= token_max_len and not token.startswith('_')\n",
    "    ]\n",
    "\n",
    "def init_to_ignore_interrupt():\n",
    "    \"\"\"Enables interruption ignoring.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    Should only be used when master is prepared to handle termination of\n",
    "    child processes.\n",
    "\n",
    "    \"\"\"\n",
    "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "\n",
    "def _process_article(args):\n",
    "    \"\"\"Same as :func:`~gensim.corpora.wikicorpus.process_article`, but with args in list format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : [(str, bool, str, int), (function, int, int, bool)]\n",
    "        First element - same as `args` from :func:`~gensim.corpora.wikicorpus.process_article`,\n",
    "        second element is tokenizer function, token minimal length, token maximal length, lowercase flag.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list of str, str, int)\n",
    "        List of tokens from article, title and page id.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    Should not be called explicitly. Use :func:`~gensim.corpora.wikicorpus.process_article` instead.\n",
    "\n",
    "    \"\"\"\n",
    "#     print(args[-1])\n",
    "    tokenizer_func, token_min_len, token_max_len, lower = args[-1]\n",
    "    args = args[:-1]\n",
    "\n",
    "    return process_article(\n",
    "        args, tokenizer_func=tokenizer_func, token_min_len=token_min_len,\n",
    "        token_max_len=token_max_len, lower=lower\n",
    "    )\n",
    "\n",
    "\n",
    "def process_article(args, tokenizer_func=tokenize, token_min_len=TOKEN_MIN_LEN,\n",
    "                    token_max_len=TOKEN_MAX_LEN, lower=True):\n",
    "    \"\"\"Parse a Wikipedia article, extract all tokens.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Set `tokenizer_func` (defaults is :func:`~gensim.corpora.wikicorpus.tokenize`) parameter for languages\n",
    "    like Japanese or Thai to perform better tokenization.\n",
    "    The `tokenizer_func` needs to take 4 parameters: (text: str, token_min_len: int, token_max_len: int, lower: bool).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : (str, bool, str, int)\n",
    "        Article text, lemmatize flag (if True, :func:`~gensim.utils.lemmatize` will be used), article title,\n",
    "        page identificator.\n",
    "    tokenizer_func : function\n",
    "        Function for tokenization (defaults is :func:`~gensim.corpora.wikicorpus.tokenize`).\n",
    "        Needs to have interface:\n",
    "        tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool) -> list of str.\n",
    "    token_min_len : int\n",
    "        Minimal token length.\n",
    "    token_max_len : int\n",
    "        Maximal token length.\n",
    "    lower : bool\n",
    "         Convert article text to lower case?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list of str, str, int)\n",
    "        List of tokens from article, title and page id.\n",
    "\n",
    "    \"\"\"\n",
    "    text, lemmatize, title, pageid = args\n",
    "    text = filter_wiki(text)\n",
    "    if lemmatize:\n",
    "        result = utils.lemmatize(text)\n",
    "    else:\n",
    "        result = tokenizer_func(text, token_min_len, token_max_len, lower)\n",
    "    return result, title, pageid\n",
    "\n",
    "def remove_markup(text, promote_remaining=True, simplify_links=True):\n",
    "    \"\"\"Filter out wiki markup from `text`, leaving only text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        String containing markup.\n",
    "    promote_remaining : bool\n",
    "        Whether uncaught markup should be promoted to plain text.\n",
    "    simplify_links : bool\n",
    "        Whether links should be simplified keeping only their description text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        `text` without markup.\n",
    "\n",
    "    \"\"\"\n",
    "    text = re.sub(RE_P2, '', text)  # remove the last list (=languages)\n",
    "    # the wiki markup is recursive (markup inside markup etc)\n",
    "    # instead of writing a recursive grammar, here we deal with that by removing\n",
    "    # markup in a loop, starting with inner-most expressions and working outwards,\n",
    "    # for as long as something changes.\n",
    "#     text = remove_template(text)\n",
    "#     text = remove_file(text)\n",
    "    iters = 0\n",
    "    while True:\n",
    "        old, iters = text, iters + 1\n",
    "        text = re.sub(RE_P0, '', text)  # remove comments\n",
    "        text = re.sub(RE_P1, '', text)  # remove footnotes\n",
    "        text = re.sub(RE_P9, '', text)  # remove outside links\n",
    "        text = re.sub(RE_P10, '', text)  # remove math content\n",
    "        text = re.sub(RE_P11, '', text)  # remove all remaining tags\n",
    "        text = re.sub(RE_P14, '', text)  # remove categories\n",
    "        text = re.sub(RE_P5, '\\\\3', text)  # remove urls, keep description\n",
    "\n",
    "        if simplify_links:\n",
    "            text = re.sub(RE_P6, '\\\\2', text)  # simplify links, keep description only\n",
    "        # remove table markup\n",
    "        text = text.replace(\"!!\", \"\\n|\")  # each table head cell on a separate line\n",
    "        text = text.replace(\"|-||\", \"\\n|\")  # for cases where a cell is filled with '-'\n",
    "        text = re.sub(RE_P12, '\\n', text)  # remove formatting lines\n",
    "        text = text.replace('|||', '|\\n|')  # each table cell on a separate line(where |{{a|b}}||cell-content)\n",
    "        text = text.replace('||', '\\n|')  # each table cell on a separate line\n",
    "        text = re.sub(RE_P13, '\\n', text)  # leave only cell content\n",
    "        text = re.sub(RE_P17, '\\n', text)  # remove formatting lines\n",
    "\n",
    "        # remove empty mark-up\n",
    "        text = text.replace('[]', '')\n",
    "        # stop if nothing changed between two iterations or after a fixed number of iterations\n",
    "        if old == text or iters > 2:\n",
    "            break\n",
    "\n",
    "    if promote_remaining:\n",
    "        text = text.replace('[', '').replace(']', '')  # promote all remaining markup to plain text\n",
    "\n",
    "    return text\n",
    "\n",
    "def filter_wiki(raw, promote_remaining=True, simplify_links=True):\n",
    "    \"\"\"Filter out wiki markup from `raw`, leaving only text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : str\n",
    "        Unicode or utf-8 encoded string.\n",
    "    promote_remaining : bool\n",
    "        Whether uncaught markup should be promoted to plain text.\n",
    "    simplify_links : bool\n",
    "        Whether links should be simplified keeping only their description text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        `raw` without markup.\n",
    "\n",
    "    \"\"\"\n",
    "    # parsing of the wiki markup is not perfect, but sufficient for our purposes\n",
    "    # contributions to improving this code are welcome :)\n",
    "    text = utils.to_unicode(raw, 'utf8', errors='ignore')\n",
    "    text = utils.decode_htmlentities(text)  # '&amp;nbsp;' --> '\\xa0'\n",
    "    return remove_markup(text, promote_remaining, simplify_links)\n",
    "\n",
    "def get_custom_texts(text):\n",
    "    \"\"\"Iterate over the dump, yielding a list of tokens for each article that passed\n",
    "    the length and namespace filtering.\n",
    "\n",
    "    Uses multiprocessing internally to parallelize the work and process the dump more quickly.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This iterates over the **texts**. If you want vectors, just use the standard corpus interface\n",
    "    instead of this method:\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    .. sourcecode:: pycon\n",
    "\n",
    "        >>> from gensim.test.utils import datapath\n",
    "        >>> from gensim.corpora import WikiCorpus\n",
    "        >>>\n",
    "        >>> path_to_wiki_dump = datapath(\"enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2\")\n",
    "        >>>\n",
    "        >>> for vec in WikiCorpus(path_to_wiki_dump):\n",
    "        ...     pass\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    list of str\n",
    "        If `metadata` is False, yield only list of token extracted from the article.\n",
    "    (list of str, (int, str))\n",
    "        List of tokens (extracted from the article), page id and article title otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    articles, articles_all = 0, 0\n",
    "    positions, positions_all = 0, 0\n",
    "\n",
    "    tokenization_params = (tokenize, TOKEN_MIN_LEN, TOKEN_MAX_LEN, True)\n",
    "    texts = ((text, utils.has_pattern(), 'custom_title', '1', tokenization_params),)\n",
    "#     print(texts)\n",
    "    processes = max(1, multiprocessing.cpu_count() - 1)\n",
    "    metadata = False\n",
    "    filter_articles = None\n",
    "    length = 0\n",
    "\n",
    "    pool = multiprocessing.Pool(processes, init_to_ignore_interrupt)\n",
    "\n",
    "    try:\n",
    "        # process the corpus in smaller chunks of docs, because multiprocessing.Pool\n",
    "        # is dumb and would load the entire input into RAM at once...\n",
    "        for group in utils.chunkize(texts, chunksize=10 * processes, maxsize=1):\n",
    "            for tokens, title, pageid in pool.imap(_process_article, group):\n",
    "                articles_all += 1\n",
    "                positions_all += len(tokens)\n",
    "                # article redirects and short stubs are pruned here\n",
    "                if len(tokens) < DESIRED_ARTICLE_MIN_WORDS or \\\n",
    "                        any(title.startswith(ignore + ':') for ignore in IGNORED_NAMESPACES):\n",
    "                    continue\n",
    "                articles += 1\n",
    "                positions += len(tokens)\n",
    "                if metadata:\n",
    "                    yield (tokens, (pageid, title))\n",
    "                else:\n",
    "                    yield tokens\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warn(\n",
    "            \"user terminated iteration over Wikipedia corpus after %i documents with %i positions \"\n",
    "            \"(total %i articles, %i positions before pruning articles shorter than %i words)\",\n",
    "            articles, positions, articles_all, positions_all, DESIRED_ARTICLE_MIN_WORDS\n",
    "        )\n",
    "    except PicklingError as exc:\n",
    "        raise_from(PicklingError('Can not send filtering function {} to multiprocessing, '\n",
    "                                 'make sure the function can be pickled.'.format(filter_articles)), exc)\n",
    "    else:\n",
    "        logger.info(\n",
    "            \"finished iterating over Wikipedia corpus of %i documents with %i positions \"\n",
    "            \"(total %i articles, %i positions before pruning articles shorter than %i words)\",\n",
    "            articles, positions, articles_all, positions_all, ARTICLE_MIN_WORDS\n",
    "        )\n",
    "        length = articles  # cache corpus length\n",
    "    finally:\n",
    "        pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLoggerDM(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self, path_prefix):\n",
    "        self.path_prefix = path_prefix\n",
    "        self.epoch = 0\n",
    "        self.start = datetime.datetime.now()\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        self.start = datetime.datetime.now()\n",
    "        print(self.start)\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        if (self.epoch >= 8):\n",
    "            output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n",
    "            model.save(output_path)\n",
    "        self.epoch += 1\n",
    "        print(datetime.datetime.now() - self.start)\n",
    "    \n",
    "    def on_train_begin(self, model):\n",
    "        print(\"Training for DM\")\n",
    "    \n",
    "    def on_train_end(self, model):\n",
    "        print(\"Training end for DM\")\n",
    "\n",
    "class EpochLoggerDBOW(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self, path_prefix):\n",
    "        self.epoch = 0\n",
    "        self.path_prefix = path_prefix\n",
    "        self.start = datetime.datetime.now()\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        self.start = datetime.datetime.now()\n",
    "        print(self.start)\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        if (self.epoch >= 8):\n",
    "            output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n",
    "            model.save(output_path)\n",
    "        self.epoch += 1\n",
    "        print(datetime.datetime.now() - self.start)\n",
    "    \n",
    "    def on_train_begin(self, model):\n",
    "        print(\"Training for DBOW\")\n",
    "    \n",
    "    def on_train_end(self, model):\n",
    "        print(\"Training end for DBOW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filePath = os.path.join(')\n",
    "# print(filePath)\n",
    "# x = get_tmpfile('DBOW_epoch9.model')\n",
    "# print(x)\n",
    "# print(os.getcwd())\n",
    "model_dbow = Doc2Vec.load('/home/ubuntu/Notebooks/wiki_corpus_doc2vec_exp/modelsData/DBOW_epoch9.model')\n",
    "model_dm = Doc2Vec.load('/home/ubuntu/Notebooks/wiki_corpus_doc2vec_exp/modelsData/DM_epoch9.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The epic treatise of modern economics, written by Adam Smith in 1776, was interestingly titled “An Inquiry into the Nature and Causes of the Wealth of Nations”. With India having become the fifth largest economy in the world in 2019 and aspiring to be the third largest by 2025, it is only befitting to go back to one of the foundational questions posed by Smith, “What causes wealth and prosperity of nations?” The Economic Survey 2019-20 makes a humble attempt to craft a framework of policies that can foster wealth creation in India. This inquiry is particularly critical at this stage as India aspires to become a $5 trillion economy by 2025 – an ambitious vision that should create, as Smith observed, “universal opulence which extends itself to the lowest ranks of the people.”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"Numeric representation of text documents is a challenging task in machine learning. Such a representation may be used for many purposes, for example: document retrieval, web search, spam filtering, topic modeling etc.\n",
    "However, there are not many good techniques to do this. Many tasks use the well known but simplistic method of bag of words (BOW), but outcomes will be mostly mediocre, since BOW loses many subtleties of a possible good representation, e.g consideration of word ordering.\n",
    "Latent Dirichlet Allocation (LDA) is also a common technique for topic modeling (extracting topics/keywords out of texts) but it’s very hard to tune, and results are hard to evaluate.\n",
    "In this post, I will review the doc2vec method, a concept that was presented in 2014 by Mikilov and Le in this article, which we are going to mention many times through this post. Worth to mention that Mikilov is one of the authors of word2vec as well.\n",
    "Doc2vec is a very nice technique. It’s easy to use, gives good results, and as you can understand from its name, heavily based on word2vec. so we’ll start with a short introduction about word2vec.\n",
    "word2vec\n",
    "word2vec is a well known concept, used to generate representation vectors out of words.\n",
    "There are many good tutorials online about word2vec, like this one and this one, but describing doc2vec without word2vec will miss the point, so I’ll be brief.\n",
    "In general, when you like to build some model using words, simply labeling/one-hot encoding them is a plausible way to go. However, when using such encoding, the words lose their meaning. e.g, if we encode Paris as id_4, France as id_6 and power as id_8, France will have the same relation to power as with Paris. We would prefer a representation in which France and Paris will be closer than France and power.\n",
    "The word2vec, presented in 2013 in this article, intends to give you just that: a numeric representation for each word, that will be able to capture such relations as above. this is part of a wider concept in machine learning — the feature vectors.\n",
    "Such representations, encapsulate different relations between words, like synonyms, antonyms, or analogies, such as this one\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.[1]\n",
    "\n",
    "Word2vec was created and published in 2013 by a team of researchers led by Tomas Mikolov at Google and patented.[2] The algorithm has been subsequently analysed and explained by other researchers.[3][4] Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms[1] such as latent semantic analysis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word', 'vec', 'is', 'group', 'of', 'related', 'models', 'that', 'are', 'used', 'to', 'produce', 'word', 'embeddings', 'these', 'models', 'are', 'shallow', 'two', 'layer', 'neural', 'networks', 'that', 'are', 'trained', 'to', 'reconstruct', 'linguistic', 'contexts', 'of', 'words', 'word', 'vec', 'takes', 'as', 'its', 'input', 'large', 'corpus', 'of', 'text', 'and', 'produces', 'vector', 'space', 'typically', 'of', 'several', 'hundred', 'dimensions', 'with', 'each', 'unique', 'word', 'in', 'the', 'corpus', 'being', 'assigned', 'corresponding', 'vector', 'in', 'the', 'space', 'word', 'vectors', 'are', 'positioned', 'in', 'the', 'vector', 'space', 'such', 'that', 'words', 'that', 'share', 'common', 'contexts', 'in', 'the', 'corpus', 'are', 'located', 'close', 'to', 'one', 'another', 'in', 'the', 'space', 'word', 'vec', 'was', 'created', 'and', 'published', 'in', 'by', 'team', 'of', 'researchers', 'led', 'by', 'tomas', 'mikolov', 'at', 'google', 'and', 'patented', 'the', 'algorithm', 'has', 'been', 'subsequently', 'analysed', 'and', 'explained', 'by', 'other', 'researchers', 'embedding', 'vectors', 'created', 'using', 'the', 'word', 'vec', 'algorithm', 'have', 'many', 'advantages', 'compared', 'to', 'earlier', 'algorithms', 'such', 'as', 'latent', 'semantic', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "text_load1 = [x for x in get_custom_texts(text2) ]\n",
    "text_load = text_load1[0]\n",
    "print(text_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model_dbow.infer_vector(text_load)\n",
    "y = model_dm.infer_vector(text_load, epochs=40)\n",
    "# print(x)\n",
    "# print(model_dbow.estimate_memory())\n",
    "# print(model_dbow.estimated_lookup_memory())\n",
    "# print(model_dbow.layer1_size)\n",
    "# model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Morphological analysis', 0.5724701285362244),\n",
      " ('Distributional–relational database', 0.5563048124313354),\n",
      " ('CmapTools', 0.5551131963729858),\n",
      " ('Sparse matrix-vector multiplication', 0.5522376894950867),\n",
      " ('Prototype methods', 0.5483927726745605),\n",
      " ('Word-sense induction', 0.5481163263320923),\n",
      " ('GloVe (machine learning)', 0.5481064319610596),\n",
      " ('Kernel-independent component analysis', 0.546776294708252),\n",
      " ('Vertex (computer graphics)', 0.5446189641952515),\n",
      " ('Latent semantic mapping', 0.5439185500144958)]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_dbow.docvecs.most_similar(positive = [x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('National Corpus of Polish', 0.5654904246330261),\n",
      " ('Word2vec', 0.5504993200302124),\n",
      " ('Word embedding', 0.49864524602890015),\n",
      " ('Tomas Mikolov', 0.4916972815990448),\n",
      " ('Pax Corpus', 0.49122172594070435),\n",
      " ('El Corpus', 0.47822171449661255),\n",
      " ('Bergen Corpus of London Teenage Language', 0.47560450434684753),\n",
      " ('Adam Kilgarriff', 0.4726002812385559),\n",
      " ('Persian Speech Corpus', 0.4718528389930725),\n",
      " ('Wellington Corpus of Spoken New Zealand English', 0.4697859287261963)]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_dm.docvecs.most_similar(positive = [y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Word embedding', 0.6466984748840332),\n",
      " ('Recursive neural network', 0.5964563488960266),\n",
      " ('Structured sparsity regularization', 0.5880581140518188),\n",
      " ('Parametric programming', 0.582451581954956),\n",
      " ('Algorithm selection', 0.5806574821472168),\n",
      " ('Structured prediction', 0.5804087519645691),\n",
      " ('Teknomo–Fernandez algorithm', 0.5788553953170776),\n",
      " ('Simulation-based optimization', 0.5785280466079712),\n",
      " ('Binary regression', 0.5777685642242432),\n",
      " ('Hopkins statistic', 0.576458215713501)]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_dbow.docvecs.most_similar(positive = ['Word2vec']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Word embedding', 0.6609228849411011),\n",
      " ('Language model', 0.6240352988243103),\n",
      " ('N-gram', 0.6167463660240173),\n",
      " ('Semantic folding', 0.5738292932510376),\n",
      " ('Paraphrasing (computational linguistics)', 0.5722178220748901),\n",
      " ('Semantic space', 0.5568692684173584),\n",
      " ('Statistical machine translation', 0.5510936975479126),\n",
      " ('Document-term matrix', 0.5505303740501404),\n",
      " ('Latent semantic analysis', 0.5490725040435791),\n",
      " ('Kneser–Ney smoothing', 0.547815203666687)]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_dm.docvecs.most_similar(positive = ['Word2vec']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Word2vec', 0.6609229445457458),\n",
      " ('Language model', 0.5932155847549438),\n",
      " ('Semantic space', 0.5806470513343811),\n",
      " ('GloVe (machine learning)', 0.5724257230758667),\n",
      " ('Semantic folding', 0.5698139071464539),\n",
      " ('Query understanding', 0.5652347207069397),\n",
      " ('Statistical semantics', 0.5561745166778564),\n",
      " ('Word-sense induction', 0.5468202829360962),\n",
      " ('Deeplearning4j', 0.5417449474334717),\n",
      " ('N-gram', 0.5404171943664551)]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_dm.docvecs.most_similar(positive = ['Word embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Document-term matrix', 0.6433390378952026),\n",
      " ('Vector space model', 0.5866307616233826),\n",
      " ('Concept search', 0.5842148065567017),\n",
      " ('Tf–idf', 0.5753623247146606),\n",
      " ('Full-text search', 0.5615909099578857),\n",
      " ('Concept mining', 0.5588423013687134),\n",
      " ('Semantic similarity', 0.5494738817214966),\n",
      " ('Word2vec', 0.5490725040435791),\n",
      " ('Bag-of-words model', 0.5450059175491333),\n",
      " ('Enterprise search', 0.5332350134849548)]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_dm.docvecs.most_similar(positive = ['Latent semantic analysis']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Document-term matrix', 0.7128222584724426),\n",
      " ('Okapi BM25', 0.67176353931427),\n",
      " ('Sentence extraction', 0.581932544708252),\n",
      " ('Divergence-from-randomness model', 0.5800539255142212),\n",
      " ('Latent semantic analysis', 0.5753623247146606),\n",
      " ('Vector space model', 0.5543138980865479),\n",
      " ('Document clustering', 0.5495814085006714),\n",
      " ('Bag-of-words model', 0.5433114171028137),\n",
      " ('Latent Dirichlet allocation', 0.5334317088127136),\n",
      " ('SMART Information Retrieval System', 0.5332077741622925)]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_dm.docvecs.most_similar(positive = ['Tf–idf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dbow.docvecs.most_similar(positive=[x], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11670896"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.docvecs.similarity(4772633,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autism\n",
      "Economy of India\n"
     ]
    }
   ],
   "source": [
    "print(model_dbow.docvecs.index_to_doctag(1))\n",
    "print(model_dbow.docvecs.index_to_doctag(411653))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772637\n"
     ]
    }
   ],
   "source": [
    "print(model_dbow.docvecs.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dbow.docvecs.rank(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doctag(offset=2884531, word_count=1075, doc_count=1)\n"
     ]
    }
   ],
   "source": [
    "y = model_dbow.docvecs[\"Economy of India\"]\n",
    "# print(model_dbow.docvecs.get_vector(\"Economy of India\"))\n",
    "print(model_dbow.docvecs.doctags[\"Economic Advisory Council\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "411653",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-308883c00604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_dbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloser_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m411653\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2884531\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mcloser_than\u001b[0;34m(self, entity1, entity2)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;34m\"\"\"Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mall_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0me1_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0me2_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mcloser_node_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_distances\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mall_distances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me2_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 411653"
     ]
    }
   ],
   "source": [
    "model_dbow.docvecs.closer_than(411653, 2884531)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
